!source ../variables.sql
--!source ../setup_db_objects.sql

/* -----------------------------------------------------------------------------
   EXTERNAL STAGES and DATA LOADING (AWS S3 Integration)
   Requires SYSADMIN role (for Stage) and ACCOUNTADMIN (for Integration).
   
   Features covered:
   1. Creating a Storage Integration (Secure Handshake)
   2. Creating an External Stage
   3. Connectivity Test and Data Validation
   4. Loading Data (COPY INTO)
------------------------------------------------------------------------------- */

USE WAREHOUSE &{warehouse_name};
USE DATABASE &{database_name};
USE SCHEMA &{schema_name};

/* -----------------------------------------------------------------------------
   STEP 0: AWS PRE-CONFIGURATION (Do this in AWS Console first)
   -----------------------------------------------------------------------------
   1. Create an S3 Bucket: 
      - Name: 'snowpro-core-test-bucket' (or your own unique name)
   
   2. Create an IAM Policy (e.g., 'Snowflake-S3-Access-Policy'):
      - JSON Permission:
        {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Action": ["s3:PutObject", "s3:GetObject", "s3:GetObjectVersion", "s3:DeleteObject", "s3:DeleteObjectVersion"],
                    "Resource": "arn:aws:s3:::snowpro-core-test-bucket/*"
                },
                {
                    "Effect": "Allow",
                    "Action": ["s3:ListBucket", "s3:GetBucketLocation"],
                    "Resource": "arn:aws:s3:::snowpro-core-test-bucket"
                }
            ]
        }

   3. Create an IAM Role (e.g., 'Snowflake-Access-Role'):
      - Trusted Entity: AWS Account (Select 'This account' for now, we update it later).
      - Permissions: Attach the policy created in step 2.
      - Copy the Role ARN: (e.g., arn:aws:iam::123456789:role/Snowflake-Access-Role)
------------------------------------------------------------------------------- */

/* -----------------------------------------------------------------------------
   STEP 1: CREATE THE STORAGE INTEGRATION
   * Exam Note: This object avoids hard-coding AWS keys. It uses an IAM User
     generated by Snowflake to assume the role you created in AWS.
------------------------------------------------------------------------------- */

USE ROLE ACCOUNTADMIN; -- Creating Integrations requires AccountAdmin

CREATE STORAGE INTEGRATION IF NOT EXISTS s3_int
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  ENABLED = TRUE
  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::123456789:role/Snowflake-Access-Role' -- PASTE YOUR ARN FROM STEP 0 HERE
  STORAGE_ALLOWED_LOCATIONS = ('s3://snowpro-core-test-bucket/');

/* -----------------------------------------------------------------------------
   STEP 1.5: THE HANDSHAKE (Trust Relationship)
   -----------------------------------------------------------------------------
   1. Run the command below: DESC INTEGRATION s3_int;
   2. Record these two values from the output:
      - STORAGE_AWS_IAM_USER_ARN (The hidden Snowflake User)
      - STORAGE_AWS_EXTERNAL_ID  (The password/secret ID)
      
   3. Go back to AWS IAM Console -> Roles -> 'Snowflake-Access-Role'
   4. Tab: "Trust relationships" -> "Edit trust policy"
   5. Update the JSON:
      {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Effect": "Allow",
            "Principal": { "AWS": "<PASTE STORAGE_AWS_IAM_USER_ARN>" },
            "Action": "sts:AssumeRole",
            "Condition": { "StringEquals": { "sts:ExternalId": "<PASTE STORAGE_AWS_EXTERNAL_ID>" } }
          }
        ]
      }
------------------------------------------------------------------------------- */

DESC INTEGRATION s3_int;

/* -----------------------------------------------------------------------------
   STEP 2: CREATE THE STAGE OBJECT
   Now that the handshake is done, we create the stage using the integration.
------------------------------------------------------------------------------- */

USE ROLE SYSADMIN;

CREATE STAGE IF NOT EXISTS my_s3_stage
  STORAGE_INTEGRATION = s3_int
  URL = 's3://snowpro-core-test-bucket/';

-- Verification 
SHOW STAGES LIKE 'my_s3_stage';


/* -----------------------------------------------------------------------------
   STEP 3: THE CONNECTIVITY TEST (LIST)
   The most immediate way to "test" an external stage is to list its files.
   If this fails, check your Policy (Step 0) or Trust Relationship (Step 1.5).
------------------------------------------------------------------------------- */
-- Requirement: Upload a file named 'test.csv' to your S3 bucket manually first.

LIST @&{database_name}.&{schema_name}.my_s3_stage;


/* -----------------------------------------------------------------------------
   STEP 4: PREVIEW DATA (QUERY IN PLACE)
   We can peek inside the CSV files residing in S3 without paying to load them yet.
------------------------------------------------------------------------------- */

-- Create a file format to tell Snowflake how to read the CSV
CREATE OR REPLACE FILE FORMAT &{database_name}.&{schema_name}.MY_CSV_FORMAT
    TYPE = 'CSV';

-- Select the raw CSV columns ($1, $2...) from the external stage
SELECT 
    $1,
    $2,
    $3
FROM @&{database_name}.&{schema_name}.my_s3_stage 
    (FILE_FORMAT => '&{database_name}.&{schema_name}.MY_CSV_FORMAT')
LIMIT 10;


/* -----------------------------------------------------------------------------
   STEP 5: LOAD DATA (COPY INTO)
   Now we load that external S3 data into a Snowflake table.
------------------------------------------------------------------------------- */

-- Load the data
COPY INTO &{database_name}.&{schema_name}.&{table_name}
    FROM @&{database_name}.&{schema_name}.my_s3_stage
    FILE_FORMAT = (FORMAT_NAME = '&{database_name}.&{schema_name}.MY_CSV_FORMAT');

-- Verify
SELECT * FROM &{database_name}.&{schema_name}.&{table_name} LIMIT 5;


/* -----------------------------------------------------------------------------
   CLEANUP
------------------------------------------------------------------------------- */
--DROP STAGE IF EXISTS &{database_name}.&{schema_name}.my_s3_stage;
--DROP FILE FORMAT IF EXISTS &{database_name}.&{schema_name}.MY_CSV_FORMAT;

--!source ../cleanup_db_objects.sql